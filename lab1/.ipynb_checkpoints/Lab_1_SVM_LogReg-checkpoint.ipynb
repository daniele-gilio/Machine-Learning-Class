{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cupy as cp\n",
    "\n",
    "\n",
    "def svm_inference(X, w, b):\n",
    "    \"\"\"SVM prediction of the class labels.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray, shape (m, n)\n",
    "         input features (one row per feature vector).\n",
    "    w : ndarray, shape (n,)\n",
    "         weight vector.\n",
    "    b : float\n",
    "         scalar bias.\n",
    "    Returns\n",
    "    -------\n",
    "    ndarray, shape (m,)\n",
    "        predicted labels (one per feature vector).\n",
    "    ndarray, shape (m,)\n",
    "        classification scores (one per feature vector).\n",
    "    \"\"\"\n",
    "    #logits = X @ w + b\n",
    "    logits=cp.add(cp.matmul(X, w),b)\n",
    "    labels = (logits > 0).astype(int)\n",
    "    return labels, logits\n",
    "\n",
    "\n",
    "def hinge_loss(labels, logits):\n",
    "    \"\"\"Average hinge loss.\n",
    "    Parameters\n",
    "    ----------\n",
    "    labels : ndarray, shape (m,)\n",
    "        binary target labels (0 or 1).\n",
    "    logits : ndarray, shape (m,)\n",
    "        classification scores (logits).\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        average hinge loss.\n",
    "    \"\"\"\n",
    "    loss = cp.maximum(0, 1 - (2 * labels - 1) * logits)\n",
    "    return loss.mean()\n",
    "\n",
    "\n",
    "def svm_train(X, Y, lambda_, lr=1e-3, steps=1000, init_w=None, init_b=0, lr0=1):\n",
    "    \"\"\"Train a binary SVM classifier.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray, shape (m, n)\n",
    "        training features.\n",
    "    Y : ndarray, shape (m,)\n",
    "        binary training labels.\n",
    "    lambda_ : float\n",
    "        regularization coefficient.\n",
    "    lr : float\n",
    "        learning rate\n",
    "    steps : int\n",
    "        number of training steps\n",
    "    init_w : ndarray, shape (n,)\n",
    "        initial weights (None for zero initialization)\n",
    "    init_b : float\n",
    "        initial bias\n",
    "    Returns\n",
    "    -------\n",
    "    w : ndarray, shape (n,)\n",
    "        learned weight vector.\n",
    "    b : float\n",
    "        learned bias.\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    w = (init_w if init_w is not None else cp.zeros(n))\n",
    "    b = init_b\n",
    "    C = 2*Y - 1\n",
    "    for step in range(steps):\n",
    "        lr=lr0/(step+1)**0.5\n",
    "        labels, logits = svm_inference(X, w, b)\n",
    "        hinge_diff = -C * ((C * logits) < 1)\n",
    "        #grad_w = (hinge_diff @ X) / m + lambda_ * w\n",
    "        grad_w = (cp.multiply(cp.matmul(hinge_diff, X), 1/m) + lambda_*w)\n",
    "        grad_b = hinge_diff.mean()\n",
    "        w -= cp.multiply(lr, grad_w)\n",
    "        b -= cp.multiply(lr, grad_b)\n",
    "        if (step+1)%100==0:\n",
    "            print(\"Step: \", step+1, \", Accuracy: \", (labels==Y).mean()*100)\n",
    "    return w, b\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "n=5000\n",
    "steps=2000\n",
    "train_name= \"train_\" + str(n)\n",
    "test_name=\"test_\" + str(n)\n",
    "val_name=\"val_\" + str(n)\n",
    "train_data=pd.read_csv(train_name+\".gz\", compression=\"gzip\", dtype=np.int32, sep=\" \", header=None).to_numpy()\n",
    "val_data=pd.read_csv(val_name + \".gz\", compression=\"gzip\", dtype=np.int32, sep=\" \", header=None).to_numpy()\n",
    "test_data=pd.read_csv(test_name + \".gz\", compression=\"gzip\", dtype=np.int32, sep=\" \", header=None).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  100 , Accuracy:  86.688\n",
      "Step:  200 , Accuracy:  87.69200000000001\n",
      "Step:  300 , Accuracy:  88.112\n",
      "Step:  400 , Accuracy:  88.424\n",
      "Step:  500 , Accuracy:  88.612\n",
      "Step:  600 , Accuracy:  88.828\n",
      "Step:  700 , Accuracy:  88.952\n",
      "Step:  800 , Accuracy:  89.092\n",
      "Step:  900 , Accuracy:  89.22\n",
      "Step:  1000 , Accuracy:  89.312\n",
      "Step:  1100 , Accuracy:  89.4\n",
      "Step:  1200 , Accuracy:  89.492\n",
      "Step:  1300 , Accuracy:  89.524\n",
      "Step:  1400 , Accuracy:  89.584\n",
      "Step:  1500 , Accuracy:  89.604\n",
      "Step:  1600 , Accuracy:  89.656\n",
      "Step:  1700 , Accuracy:  89.69200000000001\n",
      "Step:  1800 , Accuracy:  89.748\n",
      "Step:  1900 , Accuracy:  89.832\n",
      "Step:  2000 , Accuracy:  89.86800000000001\n",
      "Training Accuracy:  89.86\n",
      "Validation Accuracy:  86.976\n",
      "Test Accuracy:  87.088\n"
     ]
    }
   ],
   "source": [
    "X=cp.array(train_data[:,:-1])\n",
    "Y=cp.array(train_data[:,-1])\n",
    "\n",
    "w,b=svm_train(X,Y, lambda_=0.0001, steps=steps, lr0=1.)\n",
    "#print(b)\n",
    "labels,scores=svm_inference(X,w,b)\n",
    "accuracy=(labels==Y).mean()*100\n",
    "print(\"Training Accuracy: \", accuracy)\n",
    "\n",
    "X=cp.array(val_data[:,:-1])\n",
    "Y=cp.array(val_data[:,-1])\n",
    "labels,logits=svm_inference(X,w,b)\n",
    "accuracy=(labels==Y).mean()*100\n",
    "print(\"Validation Accuracy: \", accuracy)\n",
    "\n",
    "X=cp.array(test_data[:,:-1])\n",
    "Y=cp.array(test_data[:,-1])\n",
    "labels,scores=svm_inference(X,w,b)\n",
    "accuracy=(labels==Y).mean()*100\n",
    "print(\"Test Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"svm_\"+ str(n)+ \"_\"+ str(steps)+ \"_weights.txt\", cp.asnumpy(w))\n",
    "f=open(\"svm_\"+ str(n)+ \"_bias.txt\", \"w\")\n",
    "f.write(str(cp.asnumpy(b)))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logreg_inference(X, w, b):\n",
    "    \"\"\"Predict class probabilities.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray, shape (m, n)\n",
    "         input features (one row per feature vector).\n",
    "    w : ndarray, shape (n,)\n",
    "         weight vector.\n",
    "    b : float\n",
    "         scalar bias.\n",
    "    Returns\n",
    "    -------\n",
    "    ndarray, shape (m,)\n",
    "        probability estimates (one per feature vector).\n",
    "    \"\"\"\n",
    "    logits = cp.add(cp.matmul(X, w), b)\n",
    "    return 1 / (1 + cp.exp(-logits))\n",
    "\n",
    "\n",
    "def binary_cross_entropy(Y, P):\n",
    "    \"\"\"Average cross entropy.\n",
    "    Parameters\n",
    "    ----------\n",
    "    Y : ndarray, shape (m,)\n",
    "        binary target labels (0 or 1).\n",
    "    P : ndarray, shape (m,)\n",
    "        probability estimates.\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        average cross entropy.\n",
    "    \"\"\"\n",
    "    eps = 1e-3\n",
    "    P = np.clip(P, eps, 1 - eps)  # This prevents overflows\n",
    "    return -(Y * cp.log(P) + (1 - Y) * cp.log(1 - P)).mean()\n",
    "\n",
    "\n",
    "def logreg_train(X, Y, lr=1e-3, steps=1000, init_w=None, init_b=0):\n",
    "    \"\"\"Train a binary classifier based on logistic regression.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray, shape (m, n)\n",
    "        training features.\n",
    "    Y : ndarray, shape (m,)\n",
    "        binary training labels.\n",
    "    lr : float\n",
    "        learning rate\n",
    "    steps : int\n",
    "        number of training steps\n",
    "    init_w : ndarray, shape (n,)\n",
    "        initial weights (None for zero initialization)\n",
    "    init_b : float\n",
    "        initial bias\n",
    "    Returns\n",
    "    -------\n",
    "    w : ndarray, shape (n,)\n",
    "        learned weight vector.\n",
    "    b : float\n",
    "        learned bias.\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    w = (init_w if init_w is not None else cp.zeros(n))\n",
    "    b = init_b\n",
    "    for step in range(steps):\n",
    "        P = logreg_inference(X, w, b)\n",
    "        grad_w = cp.multiply(cp.matmul((P - Y), X), 1/m)\n",
    "        grad_b = (P - Y).mean()\n",
    "        w -= cp.multiply(lr ,grad_w)\n",
    "        b -= lr * grad_b\n",
    "        if step%100==0:\n",
    "            print(\"Step: \", step, \", Loss: \", binary_cross_entropy(Y,P))\n",
    "    return w, b\n",
    "\n",
    "\n",
    "def logreg_l2_train(X, Y, lambda_, lr=1e-3, steps=1000, init_w=None,\n",
    "                    init_b=0, lr0=1):\n",
    "    \"\"\"Train a binary classifier based on L2-regularized logistic regression.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray, shape (m, n)\n",
    "        training features.\n",
    "    Y : ndarray, shape (m,)\n",
    "        binary training labels.\n",
    "    lambda_ : float\n",
    "        regularization coefficient.\n",
    "    lr : float\n",
    "        learning rate.\n",
    "    steps : int\n",
    "        number of training steps.\n",
    "    init_w : ndarray, shape (n,)\n",
    "        initial weights (None for zero initialization)\n",
    "    init_b : float\n",
    "        initial bias\n",
    "    Returns\n",
    "    -------\n",
    "    w : ndarray, shape (n,)\n",
    "        learned weight vector.\n",
    "    b : float\n",
    "        learned bias.\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    w = (init_w if init_w is not None else cp.zeros(n))\n",
    "    b = init_b\n",
    "    for step in range(steps):\n",
    "        lr=lr0/(step+1)**0.5\n",
    "        P = logreg_inference(X, w, b)\n",
    "        grad_w = cp.multiply(cp.matmul((P - Y), X), 1/m)+2*lambda_*w\n",
    "        grad_b = (P - Y).mean()\n",
    "        w -= cp.multiply(lr ,grad_w)\n",
    "        b -= lr * grad_b\n",
    "        if step%1000==0:\n",
    "            print(\"Step: \", step, \", Loss: \", binary_cross_entropy(Y,P))\n",
    "    return w, b\n",
    "\n",
    "\n",
    "def logreg_l1_train(X, Y, lambda_, lr=1e-3, steps=1000, init_w=None, init_b=0, lr0=1):\n",
    "    \"\"\"Train a binary classifier based on L1-regularized logistic regression.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray, shape (m, n)\n",
    "        training features.\n",
    "    Y : ndarray, shape (m,)\n",
    "        binary training labels.\n",
    "    lambda_ : float\n",
    "        regularization coefficient.\n",
    "    lr : float\n",
    "        learning rate.\n",
    "    steps : int\n",
    "        number of training steps.\n",
    "    loss : ndarray, shape (steps,)\n",
    "        loss value after each training step.\n",
    "    Returns\n",
    "    -------\n",
    "    w : ndarray, shape (n,)\n",
    "        learned weight vector.\n",
    "    b : float\n",
    "        learned bias.\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    w = (init_w if init_w is not None else cp.zeros(n))\n",
    "    b = init_b\n",
    "    for step in range(steps):\n",
    "        lr=lr0/(step+1)**0.5\n",
    "        P = logreg_inference(X, w, b)\n",
    "        grad_w = cp.multiply(cp.matmul((P - Y), X), 1/m)+lambda_*cp.sign(w)\n",
    "        grad_b = (P - Y).mean()\n",
    "        w -= cp.multiply(lr ,grad_w)\n",
    "        b -= lr * grad_b\n",
    "        if step%100==0:\n",
    "            prob=logreg_inference(X,w,b)\n",
    "            pred=cp.asarray(prob>0.5)\n",
    "            accuracy=(pred==Y).mean()*100\n",
    "            print(\"Step: \", step, \", Accuracy: \", accuracy)\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  0 , Accuracy:  67.43599999999999\n",
      "Step:  100 , Accuracy:  84.664\n",
      "Step:  200 , Accuracy:  85.656\n",
      "Step:  300 , Accuracy:  86.16\n",
      "Step:  400 , Accuracy:  86.44\n",
      "Step:  500 , Accuracy:  86.712\n",
      "Step:  600 , Accuracy:  86.932\n",
      "Step:  700 , Accuracy:  87.092\n",
      "Step:  800 , Accuracy:  87.252\n",
      "Step:  900 , Accuracy:  87.38\n",
      "Step:  1000 , Accuracy:  87.424\n",
      "Step:  1100 , Accuracy:  87.556\n",
      "Step:  1200 , Accuracy:  87.66000000000001\n",
      "Step:  1300 , Accuracy:  87.76\n",
      "Step:  1400 , Accuracy:  87.824\n",
      "Step:  1500 , Accuracy:  87.884\n",
      "Step:  1600 , Accuracy:  87.94800000000001\n",
      "Step:  1700 , Accuracy:  88.0\n",
      "Step:  1800 , Accuracy:  88.06\n",
      "Step:  1900 , Accuracy:  88.116\n"
     ]
    }
   ],
   "source": [
    "#from pvml import logistic_regression as lg\n",
    "\n",
    "X=cp.array(train_data[:,:-1])\n",
    "Y=cp.array(train_data[:,-1])\n",
    "w,b=logreg_l1_train(X,Y, 0.,steps=steps, lr0=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"logreg_\"+ str(n)+ \"_\"+ str(steps)+ \"_weights.txt\", cp.asnumpy(w))\n",
    "f=open(\"logreg_\"+ str(n)+ \"_bias.txt\", \"w\")\n",
    "f.write(str(cp.asnumpy(b)))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  88.188\n",
      "Validation Accuracy:  86.24000000000001\n",
      "Test Accuracy:  86.304\n"
     ]
    }
   ],
   "source": [
    "#print(b)\n",
    "train_prob=(logreg_inference(X,w,b))\n",
    "\n",
    "train_pred=cp.asarray(train_prob>0.5)\n",
    "accuracy=(train_pred==Y).mean()*100\n",
    "print(\"Training Accuracy: \", accuracy)\n",
    "\n",
    "X=cp.array(val_data[:,:-1])\n",
    "Y=cp.array(val_data[:,-1])\n",
    "val_prob=logreg_inference(X,w,b)\n",
    "val_pred=cp.asarray(val_prob>0.5)\n",
    "accuracy=(val_pred==Y).mean()*100\n",
    "print(\"Validation Accuracy: \", accuracy)\n",
    "\n",
    "X=cp.array(test_data[:,:-1])\n",
    "Y=cp.array(test_data[:,-1])\n",
    "test_prob=logreg_inference(X,w,b)\n",
    "test_pred=cp.asarray(test_prob>0.5)\n",
    "accuracy=(test_pred==Y).mean()*100\n",
    "print(\"Test Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
