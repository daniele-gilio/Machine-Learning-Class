{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pvml import svm\n",
    "#import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def svm_inference(X, w, b):\n",
    "    \"\"\"SVM prediction of the class labels.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray, shape (m, n)\n",
    "         input features (one row per feature vector).\n",
    "    w : ndarray, shape (n,)\n",
    "         weight vector.\n",
    "    b : float\n",
    "         scalar bias.\n",
    "    Returns\n",
    "    -------\n",
    "    ndarray, shape (m,)\n",
    "        predicted labels (one per feature vector).\n",
    "    ndarray, shape (m,)\n",
    "        classification scores (one per feature vector).\n",
    "    \"\"\"\n",
    "    logits = X @ w + b\n",
    "    labels = (logits > 0).astype(int)\n",
    "    return labels, logits\n",
    "\n",
    "\n",
    "def hinge_loss(labels, logits):\n",
    "    \"\"\"Average hinge loss.\n",
    "    Parameters\n",
    "    ----------\n",
    "    labels : ndarray, shape (m,)\n",
    "        binary target labels (0 or 1).\n",
    "    logits : ndarray, shape (m,)\n",
    "        classification scores (logits).\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        average hinge loss.\n",
    "    \"\"\"\n",
    "    loss = np.maximum(0, 1 - (2 * labels - 1) * logits)\n",
    "    return loss.mean()\n",
    "\n",
    "\n",
    "def svm_train(X, Y, lambda_, lr=1e-3, steps=1000, init_w=None, init_b=0):\n",
    "    \"\"\"Train a binary SVM classifier.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray, shape (m, n)\n",
    "        training features.\n",
    "    Y : ndarray, shape (m,)\n",
    "        binary training labels.\n",
    "    lambda_ : float\n",
    "        regularization coefficient.\n",
    "    lr : float\n",
    "        learning rate\n",
    "    steps : int\n",
    "        number of training steps\n",
    "    init_w : ndarray, shape (n,)\n",
    "        initial weights (None for zero initialization)\n",
    "    init_b : float\n",
    "        initial bias\n",
    "    Returns\n",
    "    -------\n",
    "    w : ndarray, shape (n,)\n",
    "        learned weight vector.\n",
    "    b : float\n",
    "        learned bias.\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    w = (init_w if init_w is not None else np.zeros(n))\n",
    "    b = init_b\n",
    "    C = (2 * Y) - 1\n",
    "    for step in range(steps):\n",
    "        labels, logits = svm_inference(X, w, b)\n",
    "        hinge_diff = -C * ((C * logits) < 1)\n",
    "        grad_w = (hinge_diff @ X) / m + lambda_ * w\n",
    "        grad_b = hinge_diff.mean()\n",
    "        w -= lr * grad_w\n",
    "        b -= lr * grad_b\n",
    "        if step%1000==0:\n",
    "            print(\"Step: \", step)\n",
    "    return w, b\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=1000\n",
    "train_name= \"train_\" + str(n)\n",
    "test_name=\"test_\" + str(n)\n",
    "val_name=\"val_\" + str(n)\n",
    "train_data=np.loadtxt(train_name+\".gz\", dtype=np.int32)\n",
    "val_data=np.loadtxt(val_name + \".gz\", dtype=np.int32)\n",
    "test_data=np.loadtxt(test_name + \".gz\", dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  0\n",
      "Step:  1000\n",
      "Step:  2000\n",
      "Step:  3000\n",
      "Step:  4000\n",
      "Step:  5000\n",
      "Step:  6000\n",
      "Step:  7000\n",
      "Step:  8000\n",
      "Step:  9000\n",
      "Training Accuracy:  83.548\n",
      "Validation Accuracy:  83.63199999999999\n",
      "Test Accuracy:  83.184\n"
     ]
    }
   ],
   "source": [
    "X=train_data[:,:-1]\n",
    "Y=train_data[:,-1]\n",
    "\n",
    "w,b=svm_train(X,Y, lambda_=0., steps=10000)\n",
    "#print(b)\n",
    "labels,scores=svm_inference(X,w,b)\n",
    "accuracy=(labels==Y).mean()*100\n",
    "print(\"Training Accuracy: \", accuracy)\n",
    "\n",
    "X=val_data[:,:-1]\n",
    "Y=val_data[:,-1]\n",
    "labels,logits=svm_inference(X,w,b)\n",
    "accuracy=(labels==Y).mean()*100\n",
    "print(\"Validation Accuracy: \", accuracy)\n",
    "\n",
    "X=test_data[:,:-1]\n",
    "Y=test_data[:,-1]\n",
    "labels,scores=svm_inference(X,w,b)\n",
    "accuracy=(labels==Y).mean()*100\n",
    "print(\"Test Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def logreg_inference(X, w, b):\n",
    "    \"\"\"Predict class probabilities.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray, shape (m, n)\n",
    "         input features (one row per feature vector).\n",
    "    w : ndarray, shape (n,)\n",
    "         weight vector.\n",
    "    b : float\n",
    "         scalar bias.\n",
    "    Returns\n",
    "    -------\n",
    "    ndarray, shape (m,)\n",
    "        probability estimates (one per feature vector).\n",
    "    \"\"\"\n",
    "    logits = X @ w + b\n",
    "    return 1 / (1 + np.exp(-logits))\n",
    "\n",
    "\n",
    "def binary_cross_entropy(Y, P):\n",
    "    \"\"\"Average cross entropy.\n",
    "    Parameters\n",
    "    ----------\n",
    "    Y : ndarray, shape (m,)\n",
    "        binary target labels (0 or 1).\n",
    "    P : ndarray, shape (m,)\n",
    "        probability estimates.\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        average cross entropy.\n",
    "    \"\"\"\n",
    "    eps = 1e-3\n",
    "    P = np.clip(P, eps, 1 - eps)  # This prevents overflows\n",
    "    return -(Y * np.log(P) + (1 - Y) * np.log(1 - P)).mean()\n",
    "\n",
    "\n",
    "def logreg_train(X, Y, lr=1e-3, steps=1000, init_w=None, init_b=0):\n",
    "    \"\"\"Train a binary classifier based on logistic regression.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray, shape (m, n)\n",
    "        training features.\n",
    "    Y : ndarray, shape (m,)\n",
    "        binary training labels.\n",
    "    lr : float\n",
    "        learning rate\n",
    "    steps : int\n",
    "        number of training steps\n",
    "    init_w : ndarray, shape (n,)\n",
    "        initial weights (None for zero initialization)\n",
    "    init_b : float\n",
    "        initial bias\n",
    "    Returns\n",
    "    -------\n",
    "    w : ndarray, shape (n,)\n",
    "        learned weight vector.\n",
    "    b : float\n",
    "        learned bias.\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    w = (init_w if init_w is not None else np.zeros(n))\n",
    "    b = init_b\n",
    "    for step in range(steps):\n",
    "        P = logreg_inference(X, w, b)\n",
    "        grad_w = ((P - Y) @ X) / m\n",
    "        grad_b = (P - Y).mean()\n",
    "        w -= lr * grad_w\n",
    "        b -= lr * grad_b\n",
    "        if step%1000==0:\n",
    "            print(\"Step: \", step)\n",
    "    return w, b\n",
    "\n",
    "\n",
    "def logreg_l2_train(X, Y, lambda_, lr=1e-3, steps=1000, init_w=None,\n",
    "                    init_b=0):\n",
    "    \"\"\"Train a binary classifier based on L2-regularized logistic regression.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray, shape (m, n)\n",
    "        training features.\n",
    "    Y : ndarray, shape (m,)\n",
    "        binary training labels.\n",
    "    lambda_ : float\n",
    "        regularization coefficient.\n",
    "    lr : float\n",
    "        learning rate.\n",
    "    steps : int\n",
    "        number of training steps.\n",
    "    init_w : ndarray, shape (n,)\n",
    "        initial weights (None for zero initialization)\n",
    "    init_b : float\n",
    "        initial bias\n",
    "    Returns\n",
    "    -------\n",
    "    w : ndarray, shape (n,)\n",
    "        learned weight vector.\n",
    "    b : float\n",
    "        learned bias.\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    w = (init_w if init_w is not None else np.zeros(n))\n",
    "    b = init_b\n",
    "    for step in range(steps):\n",
    "        P = logreg_inference(X, w, b)\n",
    "        grad_w = ((P - Y) @ X) / m + 2 * lambda_ * w\n",
    "        grad_b = (P - Y).mean()\n",
    "        w -= lr * grad_w\n",
    "        b -= lr * grad_b\n",
    "    return w, b\n",
    "\n",
    "\n",
    "def logreg_l1_train(X, Y, lambda_, lr=1e-3, steps=1000, init_w=None, init_b=0):\n",
    "    \"\"\"Train a binary classifier based on L1-regularized logistic regression.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray, shape (m, n)\n",
    "        training features.\n",
    "    Y : ndarray, shape (m,)\n",
    "        binary training labels.\n",
    "    lambda_ : float\n",
    "        regularization coefficient.\n",
    "    lr : float\n",
    "        learning rate.\n",
    "    steps : int\n",
    "        number of training steps.\n",
    "    loss : ndarray, shape (steps,)\n",
    "        loss value after each training step.\n",
    "    Returns\n",
    "    -------\n",
    "    w : ndarray, shape (n,)\n",
    "        learned weight vector.\n",
    "    b : float\n",
    "        learned bias.\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    w = (init_w if init_w is not None else np.zeros(n))\n",
    "    b = init_b\n",
    "    for step in range(steps):\n",
    "        P = logreg_inference(X, w, b)\n",
    "        grad_w = ((P - Y) @ X) / m + lambda_ * np.sign(w)\n",
    "        grad_b = (P - Y).mean()\n",
    "        w -= lr * grad_w\n",
    "        b -= lr * grad_b\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  0\n",
      "Step:  1000\n",
      "Step:  2000\n",
      "Step:  3000\n",
      "Step:  4000\n",
      "Step:  5000\n",
      "Step:  6000\n",
      "Step:  7000\n",
      "Step:  8000\n",
      "Step:  9000\n",
      "Training Accuracy:  81.624\n",
      "Validation Accuracy:  81.336\n",
      "Test Accuracy:  81.22399999999999\n"
     ]
    }
   ],
   "source": [
    "#from pvml import logistic_regression as lg\n",
    "\n",
    "X=train_data[:,:-1]\n",
    "Y=train_data[:,-1]\n",
    "w,b=logreg_train(X,Y, steps=10000)\n",
    "#print(b)\n",
    "train_prob=logreg_inference(X,w,b)\n",
    "train_pred=np.asarray(train_prob>0.5)\n",
    "accuracy=(train_pred==Y).mean()*100\n",
    "print(\"Training Accuracy: \", accuracy)\n",
    "\n",
    "X=val_data[:,:-1]\n",
    "Y=val_data[:,-1]\n",
    "val_prob=logreg_inference(X,w,b)\n",
    "val_pred=np.asarray(val_prob>0.5)\n",
    "accuracy=(val_pred==Y).mean()*100\n",
    "print(\"Validation Accuracy: \", accuracy)\n",
    "\n",
    "X=test_data[:,:-1]\n",
    "Y=test_data[:,-1]\n",
    "test_prob=logreg_inference(X,w,b)\n",
    "test_pred=np.asarray(test_prob>0.5)\n",
    "accuracy=(test_pred==Y).mean()*100\n",
    "print(\"Test Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
